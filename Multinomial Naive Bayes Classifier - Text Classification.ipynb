{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# To work with files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = r\".\\20_newsgroups\"\n",
    "# The address of the base folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [f for f in os.listdir(base) if not f.startswith('.')]\n",
    "folders\n",
    "# These are the different folders in the 20_newsgroups directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_words = ['subject:','from:', 'date:', 'newsgroups:', 'message-id:', 'lines:', 'path:', 'organization:', \n",
    "            'would', 'writes:', 'references:', 'article', 'sender:', 'nntp-posting-host:', 'people', \n",
    "            'university', 'think', 'xref:', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution:', 'first', \n",
    "            'anyone','world', 'really', 'since', 'right', 'believe', 'still', \n",
    "            \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\",'newsgroups', 'xref', 'path', \n",
    "            'from', 'subject', 'sender', 'organisation', 'apr','gmt', 'last','better','never','every','even','two',\n",
    "            'good','used','first','need','going','must','really','might','well','without','made','give','look',\n",
    "            'try','far','less','seem','new','make','many','way','since','using','take','help','thanks','send',\n",
    "            'free','may','see','much','want','find','would','one','like','get','use','also','could','say','us',\n",
    "            'go','please','said','set','got','sure','come','lot','seems','able','anything','put', '--', '|>', '>>',\n",
    "            '93', 'xref', 'cantaloupe.srv.cs.cmu.edu', '20', '16', '21', '19', '10', '17', '24', 'reply-to:', 'thu',\n",
    "            'nntp-posting-host:', 're:','25''18'\"i'd\"'>i''22''fri,''23''>the','references:','xref:','sender:',\n",
    "            'writes:','1993']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "stop_words += block_words\n",
    "stop_words += list(string.punctuation)\n",
    "# We create a list of stop words and block words, i.e words that we won't use in our vocaubulary, \n",
    "# because they are very common, or do not help us\n",
    "# We will also add punctuations in stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(base, folder))\n",
    "    for file in files:\n",
    "        file_path = os.path.join(base, folder, file)\n",
    "        with open(file_path, 'r', errors='ignore') as fileobj:\n",
    "            # We go through all the files\n",
    "            data.append((' '.join([word.lower() for word in fileobj.read().strip().split() if\n",
    "                                  not word.lower() in stop_words and len(word.lower()) > 1]), folder))\n",
    "            # For every file, we remove the stop words, single words, and extra spaces\n",
    "            # Then join the text back together and store it in an n x 2 array for all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data, columns = ['text', 'category'])\n",
    "# We convert it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism:49960 alt.atheism.moderated:713 ne...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism:51060 alt.atheism.moderated:727 ne...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism cantaloupe.srv.cs.cmu.edu!crabappl...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism:51120 alt.politics.usa.constitutio...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism:51121 soc.motss:139944 rec.scoutin...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>alt.atheism:54482 talk.religion.misc:84566 alt...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>alt.atheism:54485 talk.religion.misc:84567 tal...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>talk.religion.misc:84568 talk.politics.misc:18...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>talk.religion.misc:84569 talk.religion.newage:...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>talk.abortion:121820 alt.atheism:54486 talk.re...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text            category\n",
       "0      alt.atheism:49960 alt.atheism.moderated:713 ne...         alt.atheism\n",
       "1      alt.atheism:51060 alt.atheism.moderated:727 ne...         alt.atheism\n",
       "2      alt.atheism cantaloupe.srv.cs.cmu.edu!crabappl...         alt.atheism\n",
       "3      alt.atheism:51120 alt.politics.usa.constitutio...         alt.atheism\n",
       "4      alt.atheism:51121 soc.motss:139944 rec.scoutin...         alt.atheism\n",
       "...                                                  ...                 ...\n",
       "19992  alt.atheism:54482 talk.religion.misc:84566 alt...  talk.religion.misc\n",
       "19993  alt.atheism:54485 talk.religion.misc:84567 tal...  talk.religion.misc\n",
       "19994  talk.religion.misc:84568 talk.politics.misc:18...  talk.religion.misc\n",
       "19995  talk.religion.misc:84569 talk.religion.newage:...  talk.religion.misc\n",
       "19996  talk.abortion:121820 alt.atheism:54486 talk.re...  talk.religion.misc\n",
       "\n",
       "[19997 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_df, test_df = model_selection.train_test_split(data_df, random_state = 0)\n",
    "# Splitting the data into two different dataframes, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14997, 2) (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.motorcycles             284\n",
       "talk.politics.mideast       281\n",
       "rec.autos                   269\n",
       "misc.forsale                261\n",
       "talk.politics.misc          259\n",
       "sci.med                     256\n",
       "comp.graphics               253\n",
       "soc.religion.christian      252\n",
       "comp.os.ms-windows.misc     249\n",
       "talk.politics.guns          249\n",
       "rec.sport.baseball          248\n",
       "sci.space                   246\n",
       "sci.electronics             244\n",
       "comp.windows.x              240\n",
       "comp.sys.ibm.pc.hardware    240\n",
       "talk.religion.misc          236\n",
       "comp.sys.mac.hardware       236\n",
       "sci.crypt                   233\n",
       "alt.atheism                 233\n",
       "rec.sport.hockey            231\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['category'].value_counts()\n",
    "# We can see that the data is evenly divided, Which will be helpful later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary (From training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_basic = {}\n",
    "for row in train_df.values:\n",
    "    for word in row[0].split():\n",
    "        vocab_basic[word] = vocab_basic.get(word, 0) + 1\n",
    "        # We go through our training data, and build a dictionary of all the words and their occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest \n",
    "num_features = 2500\n",
    "most_common_words = nlargest(num_features, vocab_basic, key = vocab_basic.get) \n",
    "# Next we use a heap to get the top 2500 most used words,\n",
    "# We will use these words as features for our data\n",
    "vocab = { word:vocab_basic[word] for word in most_common_words} \n",
    "# We build the final vocabulary (as a dictionary named vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 of the most common words: \n",
      "\n",
      "\n",
      "know  \t\t 5602\n",
      "i'm  \t\t 4314\n",
      "time  \t\t 3261\n",
      "it.  \t\t 3127\n",
      "computer  \t\t 2375\n",
      "something  \t\t 2345\n",
      "i've  \t\t 2319\n",
      "system  \t\t 2245\n",
      "god  \t\t 2239\n",
      "15  \t\t 2177\n",
      "news  \t\t 2142\n",
      "back  \t\t 2112\n",
      "can't  \t\t 2098\n",
      "state  \t\t 2095\n",
      "work  \t\t 2009\n",
      "someone  \t\t 1978\n",
      ">in  \t\t 1940\n",
      "23  \t\t 1883\n",
      "problem  \t\t 1878\n",
      "government  \t\t 1870\n",
      "another  \t\t 1865\n",
      "information  \t\t 1865\n",
      "read  \t\t 1862\n",
      "usa  \t\t 1847\n",
      ">the  \t\t 1831\n",
      "number  \t\t 1812\n",
      "things  \t\t 1782\n",
      "that's  \t\t 1771\n",
      "part  \t\t 1753\n",
      "22  \t\t 1727\n",
      "point  \t\t 1714\n",
      ">i  \t\t 1708\n",
      "tue,  \t\t 1704\n",
      "little  \t\t 1701\n",
      "fri,  \t\t 1685\n",
      "windows  \t\t 1665\n",
      "file  \t\t 1607\n",
      "data  \t\t 1579\n",
      "probably  \t\t 1575\n",
      "years  \t\t 1575\n",
      "space  \t\t 1570\n",
      "long  \t\t 1550\n",
      "question  \t\t 1547\n",
      "tell  \t\t 1538\n",
      "(usenet  \t\t 1535\n",
      "different  \t\t 1530\n",
      "around  \t\t 1524\n",
      "public  \t\t 1523\n",
      "available  \t\t 1521\n",
      "it,  \t\t 1518\n"
     ]
    }
   ],
   "source": [
    "print('50 of the most common words: \\n\\n')\n",
    "for i in list(vocab.keys())[:50]:\n",
    "    print(i, ' \\t\\t',vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into 2D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to figure out the columns of the 2D array\n",
    "cols = list(vocab.keys())\n",
    "cols.sort()\n",
    "# print(columns)\n",
    "# Now we have the columns (in sorted order), Let us create the 2D matrix of x\n",
    "# Our output classes will be from the folders array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(text,category,features):\n",
    "    # We get the text, the category and the features used\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(text)):\n",
    "        current_x = [0 for _i in range(len(features))]\n",
    "        for word in text[i].split():\n",
    "            # We check if the word is a column/feature in our 2D array, If it is then we increment is by 1 at that index\n",
    "            if word in features:\n",
    "                current_x[features.index(word)] += 1 # Go to the column where we have kept that word, and increment it\n",
    "        X.append(current_x) # Add the current row (for current file) to the main 2D matrix\n",
    "        Y.append(category[i]) # Append the class for current file\n",
    "    np_x = np.array(X, dtype = int)\n",
    "    np_y = np.array(Y, dtype = str)\n",
    "    return np_x, np_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = convert_data(train_df['text'].values, train_df['category'].values, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,  y_test  = convert_data(test_df['text'].values,  test_df['category'].values,  cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"a</th>\n",
       "      <th>\"i</th>\n",
       "      <th>\"if</th>\n",
       "      <th>\"in</th>\n",
       "      <th>\"it</th>\n",
       "      <th>\"the</th>\n",
       "      <th>\"we</th>\n",
       "      <th>\"what</th>\n",
       "      <th>\"you</th>\n",
       "      <th>#1</th>\n",
       "      <th>...</th>\n",
       "      <th>yet,</th>\n",
       "      <th>yet.</th>\n",
       "      <th>york</th>\n",
       "      <th>york,</th>\n",
       "      <th>you,</th>\n",
       "      <th>you.</th>\n",
       "      <th>you?</th>\n",
       "      <th>young</th>\n",
       "      <th>yourself.</th>\n",
       "      <th>||</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14997 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       \"a  \"i  \"if  \"in  \"it  \"the  \"we  \"what  \"you  #1  ...  yet,  yet.  \\\n",
       "0       0   1    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "1       0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "2       0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "3       0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "4       0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "...    ..  ..  ...  ...  ...   ...  ...    ...   ...  ..  ...   ...   ...   \n",
       "14992   0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "14993   0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "14994   0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "14995   0   0    0    0    0     0    0      0     0   1  ...     0     0   \n",
       "14996   0   0    0    0    0     0    0      0     0   0  ...     0     0   \n",
       "\n",
       "       york  york,  you,  you.  you?  young  yourself.  ||  \n",
       "0         0      0     0     0     0      0          0   0  \n",
       "1         0      0     0     0     0      0          0   0  \n",
       "2         0      0     0     0     0      0          0   0  \n",
       "3         0      0     0     0     0      0          0   0  \n",
       "4         0      0     0     0     0      0          0   0  \n",
       "...     ...    ...   ...   ...   ...    ...        ...  ..  \n",
       "14992     1      1     1     0     0      0          0   0  \n",
       "14993     0      0     0     0     0      0          0   0  \n",
       "14994     0      0     0     0     0      0          0   0  \n",
       "14995     0      0     0     0     0      0          0   0  \n",
       "14996     0      0     0     0     0      0          0   0  \n",
       "\n",
       "[14997 rows x 2500 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have now converted the data into a x, a 2D array , and y, the categories\n",
    "# The features used are top 2500 'most common words'\n",
    "# Let us visualize our 2D Array\n",
    "df_xtrain = pd.DataFrame(x_train, columns = cols)\n",
    "df_xtrain\n",
    "# Most of the data is zero,since this dataframe is made from a sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Multinomial Naive Bayes from SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score\t 0.8979129159165167 \n",
      "Test Score\t 0.8432\n"
     ]
    }
   ],
   "source": [
    "# Using the inbuild Multinomial Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred_sklearn = clf.predict(x_test)\n",
    "train_score = clf.score(x_train, y_train)\n",
    "test_score = clf.score(x_test, y_test)\n",
    "print(\"Train Score\\t\", train_score, \"\\nTest Score\\t\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Multinomial Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    # Dictionary that we will use to calculate the probabilities while predicting\n",
    "    result = {}\n",
    "    result['total_data'] = y_train.shape[0]\n",
    "    for current_class in set(y_train):\n",
    "        # Since we need to go over unique values only, we use a set\n",
    "        result[current_class] = {}\n",
    "        \n",
    "        # Lets get the part of array which is useful\n",
    "        x_train_current = x_train[y_train == current_class]\n",
    "        y_train_current = y_train[y_train == current_class]\n",
    "        \n",
    "        # Lets keep a count of total_words, since we need to store that value as well\n",
    "        total_words = 0\n",
    "        for j in range(num_features):\n",
    "            # Now we go over all the features, and keep a sum of the words\n",
    "            result[current_class][j] = x_train_current[:,j].sum()\n",
    "            total_words += result[current_class][j]\n",
    "        # Total_count keeps the track of total words, for an entire category\n",
    "        result[current_class]['total_count'] = total_words\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProb(current_class, x, dictionary):\n",
    "    # We initialize the ans with log probability of p(class = current_class) \n",
    "    ans = np.log(dictionary[current_class]['total_count']) - np.log(dictionary['total_data'])\n",
    "    # We go over all the features\n",
    "    for i in range(num_features):\n",
    "        # We calculate the occurance of current word\n",
    "        current_word = dictionary[current_class][i] + 1\n",
    "        # And the occurance of total words\n",
    "        total_words = dictionary[current_class]['total_count'] + len(x)\n",
    "        # And calculate the log probability of the current word\n",
    "        current_word_prob = np.log(current_word) - np.log(total_words)\n",
    "        # We add the probability of current word, as many times as the word occurs\n",
    "        for j in range(x[i]):\n",
    "            ans += current_word_prob\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(x, dictionary):\n",
    "    best_class = None\n",
    "    best_prob = None\n",
    "    first = True\n",
    "    all_classes = dictionary.keys()\n",
    "    # We go over all the classes \n",
    "    for current_class in all_classes:\n",
    "        if current_class == 'total_data':\n",
    "            continue\n",
    "        # We then calculate the probability of each class\n",
    "        current_prob = getProb(current_class, x, dictionary)\n",
    "        # We choose the class with the highest probability\n",
    "        if first is True or current_prob > best_prob:\n",
    "            best_prob = current_prob\n",
    "            best_class = current_class\n",
    "        first = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, dictionary):\n",
    "    y_predicted = []\n",
    "    for current_x in x_test:\n",
    "        # We go over all the dataset one by one and predict the output for every single point\n",
    "        y_predicted.append(predictSinglePoint(current_x, dictionary))\n",
    "    return np.array(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on the given data\n",
    "dictionary = fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting using the model\n",
    "y_pred_self = predict(x_test, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision between Sklearn Multinomial Naive Bayes and Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for sklearn MultinomialNB()                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.72      0.81      0.76       233\n",
      "           comp.graphics       0.79      0.76      0.77       253\n",
      " comp.os.ms-windows.misc       0.82      0.87      0.84       249\n",
      "comp.sys.ibm.pc.hardware       0.82      0.85      0.84       240\n",
      "   comp.sys.mac.hardware       0.86      0.90      0.88       236\n",
      "          comp.windows.x       0.91      0.81      0.86       240\n",
      "            misc.forsale       0.83      0.84      0.83       261\n",
      "               rec.autos       0.86      0.91      0.89       269\n",
      "         rec.motorcycles       0.88      0.94      0.91       284\n",
      "      rec.sport.baseball       0.92      0.98      0.95       248\n",
      "        rec.sport.hockey       0.96      0.94      0.95       231\n",
      "               sci.crypt       0.94      0.90      0.92       233\n",
      "         sci.electronics       0.81      0.86      0.83       244\n",
      "                 sci.med       0.94      0.87      0.90       256\n",
      "               sci.space       0.90      0.91      0.90       246\n",
      "  soc.religion.christian       0.92      0.98      0.95       252\n",
      "      talk.politics.guns       0.73      0.84      0.79       249\n",
      "   talk.politics.mideast       0.91      0.85      0.88       281\n",
      "      talk.politics.misc       0.71      0.59      0.64       259\n",
      "      talk.religion.misc       0.61      0.42      0.50       236\n",
      "\n",
      "                accuracy                           0.84      5000\n",
      "               macro avg       0.84      0.84      0.84      5000\n",
      "            weighted avg       0.84      0.84      0.84      5000\n",
      "\n",
      "Classification report for self-implemented Naive Bayes                            precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.72      0.81      0.76       233\n",
      "           comp.graphics       0.79      0.77      0.78       253\n",
      " comp.os.ms-windows.misc       0.82      0.87      0.84       249\n",
      "comp.sys.ibm.pc.hardware       0.82      0.86      0.84       240\n",
      "   comp.sys.mac.hardware       0.87      0.90      0.88       236\n",
      "          comp.windows.x       0.90      0.82      0.86       240\n",
      "            misc.forsale       0.84      0.82      0.83       261\n",
      "               rec.autos       0.87      0.91      0.89       269\n",
      "         rec.motorcycles       0.88      0.94      0.91       284\n",
      "      rec.sport.baseball       0.92      0.98      0.95       248\n",
      "        rec.sport.hockey       0.96      0.94      0.95       231\n",
      "               sci.crypt       0.93      0.90      0.91       233\n",
      "         sci.electronics       0.83      0.87      0.85       244\n",
      "                 sci.med       0.94      0.87      0.90       256\n",
      "               sci.space       0.89      0.91      0.90       246\n",
      "  soc.religion.christian       0.92      0.98      0.95       252\n",
      "      talk.politics.guns       0.73      0.85      0.79       249\n",
      "   talk.politics.mideast       0.90      0.85      0.87       281\n",
      "      talk.politics.misc       0.70      0.59      0.64       259\n",
      "      talk.religion.misc       0.61      0.42      0.50       236\n",
      "\n",
      "                accuracy                           0.84      5000\n",
      "               macro avg       0.84      0.84      0.84      5000\n",
      "            weighted avg       0.84      0.84      0.84      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification report for sklearn MultinomialNB()\",classification_report(y_test, y_pred_sklearn))\n",
    "print(\"Classification report for self-implemented Naive Bayes \",classification_report(y_test, y_pred_self))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
